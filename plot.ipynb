{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.models import BaseCMNModel\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_agrs():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data input settings\n",
    "    parser.add_argument('--image_dir', type=str, default='F:/radiologyReportGeneration/datasets/iu_xray/images/',\n",
    "                        help='the path to the directory containing the data.')\n",
    "    parser.add_argument('--ann_path', type=str, default='F:/radiologyReportGeneration/datasets/iu_xray/annotation.json',\n",
    "                        help='the path to the directory containing the data.')\n",
    "\n",
    "    # IDAM\n",
    "    parser.add_argument(\"--useIDAM\", action=\"store_false\", help=\"do you use IDAM?\")\n",
    "\n",
    "    # Mamba\n",
    "    parser.add_argument(\"--useVTFCM\", action=\"store_false\", help=\"do you use MAM?\")\n",
    "\n",
    "    # Data loader settings\n",
    "    parser.add_argument('--dataset_name', type=str, default='iu_xray', choices=['iu_xray', 'mimic_cxr'],\n",
    "                        help='the dataset to be used.')\n",
    "    parser.add_argument('--max_seq_length', type=int, default=60, help='the maximum sequence length of the reports.')\n",
    "    parser.add_argument('--threshold', type=int, default=3, help='the cut off frequency for the words.')\n",
    "    parser.add_argument('--num_workers', type=int, default=2, help='the number of workers for dataloader.')\n",
    "    parser.add_argument('--batch_size', type=int, default=2, help='the number of samples for a batch')\n",
    "\n",
    "    # Model settings (for visual extractor)\n",
    "    parser.add_argument('--visual_extractor', type=str, default='resnet101', help='the visual extractor to be used.')\n",
    "    parser.add_argument('--visual_extractor_pretrained', type=bool, default=True, help='whether to load the pretrained visual extractor')\n",
    "\n",
    "    # Model settings (for Transformer)\n",
    "    parser.add_argument('--d_model', type=int, default=512, help='the dimension of Transformer.')\n",
    "    parser.add_argument('--d_ff', type=int, default=512, help='the dimension of FFN.')\n",
    "    parser.add_argument('--d_vf', type=int, default=2048, help='the dimension of the patch features.')\n",
    "    parser.add_argument('--num_heads', type=int, default=8, help='the number of heads in Transformer.')\n",
    "    parser.add_argument('--num_layers', type=int, default=3, help='the number of layers of Transformer.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.1, help='the dropout rate of Transformer.')\n",
    "    parser.add_argument('--logit_layers', type=int, default=1, help='the number of the logit layer.')\n",
    "    parser.add_argument('--bos_idx', type=int, default=0, help='the index of <bos>.')\n",
    "    parser.add_argument('--eos_idx', type=int, default=0, help='the index of <eos>.')\n",
    "    parser.add_argument('--pad_idx', type=int, default=0, help='the index of <pad>.')\n",
    "    parser.add_argument('--use_bn', type=int, default=0, help='whether to use batch normalization.')\n",
    "    parser.add_argument('--drop_prob_lm', type=float, default=0.5, help='the dropout rate of the output layer.')\n",
    "\n",
    "    # for Cross-modal Memory\n",
    "    parser.add_argument('--topk', type=int, default=32, help='the number of k.')\n",
    "    parser.add_argument('--cmm_size', type=int, default=2048, help='the numebr of cmm size.')\n",
    "    parser.add_argument('--cmm_dim', type=int, default=512, help='the dimension of cmm dimension.')\n",
    "\n",
    "    # Sample related\n",
    "    parser.add_argument('--sample_method', type=str, default='beam_search', help='the sample methods to sample a report.')\n",
    "    parser.add_argument('--beam_size', type=int, default=3, help='the beam size when beam searching.')\n",
    "    parser.add_argument('--temperature', type=float, default=1.0, help='the temperature when sampling.')\n",
    "    parser.add_argument('--sample_n', type=int, default=1, help='the sample number per image.')\n",
    "    parser.add_argument('--group_size', type=int, default=1, help='the group size.')\n",
    "    parser.add_argument('--output_logsoftmax', type=int, default=1, help='whether to output the probabilities.')\n",
    "    parser.add_argument('--decoding_constraint', type=int, default=0, help='whether decoding constraint.')\n",
    "    parser.add_argument('--block_trigrams', type=int, default=1, help='whether to use block trigrams.')\n",
    "\n",
    "    # Trainer settings\n",
    "    parser.add_argument('--n_gpu', type=int, default=1, help='the number of gpus to be used.')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='the number of training epochs.')\n",
    "    parser.add_argument('--save_dir', type=str, default='results/iu_xray/', help='the patch to save the models.')\n",
    "    parser.add_argument('--record_dir', type=str, default='records/', help='the patch to save the results of experiments.')\n",
    "    parser.add_argument('--log_period', type=int, default=50, help='the logging interval (in batches).')\n",
    "    parser.add_argument('--save_period', type=int, default=10, help='the saving period (in epochs).')\n",
    "    parser.add_argument('--monitor_mode', type=str, default='max', choices=['min', 'max'], help='whether to max or min the metric.')\n",
    "    parser.add_argument('--monitor_metric', type=str, default='BLEU_4', help='the metric to be monitored.')\n",
    "    parser.add_argument('--early_stop', type=int, default=50, help='the patience of training.')\n",
    "\n",
    "    # Optimization\n",
    "    parser.add_argument('--optim', type=str, default='Adam', help='the type of the optimizer.')\n",
    "    parser.add_argument('--lr_ve', type=float, default=1e-4, help='the learning rate for the visual extractor.')\n",
    "    parser.add_argument('--lr_ed', type=float, default=5e-4, help='the learning rate for the remaining parameters.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-5, help='the weight decay.')\n",
    "    parser.add_argument('--adam_betas', type=tuple, default=(0.9, 0.98), help='the weight decay.')\n",
    "    parser.add_argument('--adam_eps', type=float, default=1e-9, help='the weight decay.')\n",
    "    parser.add_argument('--amsgrad', type=bool, default=True, help='.')\n",
    "    parser.add_argument('--noamopt_warmup', type=int, default=5000, help='.')\n",
    "    parser.add_argument('--noamopt_factor', type=int, default=1, help='.')\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    parser.add_argument('--lr_scheduler', type=str, default='StepLR', help='the type of the learning rate scheduler.')\n",
    "    parser.add_argument('--step_size', type=int, default=10, help='the step size of the learning rate scheduler.')\n",
    "    parser.add_argument('--gamma', type=float, default=0.8, help='the gamma of the learning rate scheduler.')\n",
    "\n",
    "    # Others\n",
    "    parser.add_argument('--seed', type=int, default=9233, help='.')\n",
    "    parser.add_argument('--resume', type=str, help='whether to resume the training from existing checkpoints.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'state_dict', 'optimizer', 'monitor_best'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.load(\"F:/radiologyReportGeneration/result/CMN+IFAM+CS/model_best_1.pth\")\n",
    "weight.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight = weight.get('state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['visual_extractor.model.0.weight', 'visual_extractor.model.1.weight', 'visual_extractor.model.1.bias', 'visual_extractor.model.1.running_mean', 'visual_extractor.model.1.running_var', 'visual_extractor.model.1.num_batches_tracked', 'visual_extractor.model.4.0.conv1.weight', 'visual_extractor.model.4.0.bn1.weight', 'visual_extractor.model.4.0.bn1.bias', 'visual_extractor.model.4.0.bn1.running_mean', 'visual_extractor.model.4.0.bn1.running_var', 'visual_extractor.model.4.0.bn1.num_batches_tracked', 'visual_extractor.model.4.0.conv2.weight', 'visual_extractor.model.4.0.bn2.weight', 'visual_extractor.model.4.0.bn2.bias', 'visual_extractor.model.4.0.bn2.running_mean', 'visual_extractor.model.4.0.bn2.running_var', 'visual_extractor.model.4.0.bn2.num_batches_tracked', 'visual_extractor.model.4.0.conv3.weight', 'visual_extractor.model.4.0.bn3.weight', 'visual_extractor.model.4.0.bn3.bias', 'visual_extractor.model.4.0.bn3.running_mean', 'visual_extractor.model.4.0.bn3.running_var', 'visual_extractor.model.4.0.bn3.num_batches_tracked', 'visual_extractor.model.4.0.downsample.0.weight', 'visual_extractor.model.4.0.downsample.1.weight', 'visual_extractor.model.4.0.downsample.1.bias', 'visual_extractor.model.4.0.downsample.1.running_mean', 'visual_extractor.model.4.0.downsample.1.running_var', 'visual_extractor.model.4.0.downsample.1.num_batches_tracked', 'visual_extractor.model.4.1.conv1.weight', 'visual_extractor.model.4.1.bn1.weight', 'visual_extractor.model.4.1.bn1.bias', 'visual_extractor.model.4.1.bn1.running_mean', 'visual_extractor.model.4.1.bn1.running_var', 'visual_extractor.model.4.1.bn1.num_batches_tracked', 'visual_extractor.model.4.1.conv2.weight', 'visual_extractor.model.4.1.bn2.weight', 'visual_extractor.model.4.1.bn2.bias', 'visual_extractor.model.4.1.bn2.running_mean', 'visual_extractor.model.4.1.bn2.running_var', 'visual_extractor.model.4.1.bn2.num_batches_tracked', 'visual_extractor.model.4.1.conv3.weight', 'visual_extractor.model.4.1.bn3.weight', 'visual_extractor.model.4.1.bn3.bias', 'visual_extractor.model.4.1.bn3.running_mean', 'visual_extractor.model.4.1.bn3.running_var', 'visual_extractor.model.4.1.bn3.num_batches_tracked', 'visual_extractor.model.4.2.conv1.weight', 'visual_extractor.model.4.2.bn1.weight', 'visual_extractor.model.4.2.bn1.bias', 'visual_extractor.model.4.2.bn1.running_mean', 'visual_extractor.model.4.2.bn1.running_var', 'visual_extractor.model.4.2.bn1.num_batches_tracked', 'visual_extractor.model.4.2.conv2.weight', 'visual_extractor.model.4.2.bn2.weight', 'visual_extractor.model.4.2.bn2.bias', 'visual_extractor.model.4.2.bn2.running_mean', 'visual_extractor.model.4.2.bn2.running_var', 'visual_extractor.model.4.2.bn2.num_batches_tracked', 'visual_extractor.model.4.2.conv3.weight', 'visual_extractor.model.4.2.bn3.weight', 'visual_extractor.model.4.2.bn3.bias', 'visual_extractor.model.4.2.bn3.running_mean', 'visual_extractor.model.4.2.bn3.running_var', 'visual_extractor.model.4.2.bn3.num_batches_tracked', 'visual_extractor.model.5.0.conv1.weight', 'visual_extractor.model.5.0.bn1.weight', 'visual_extractor.model.5.0.bn1.bias', 'visual_extractor.model.5.0.bn1.running_mean', 'visual_extractor.model.5.0.bn1.running_var', 'visual_extractor.model.5.0.bn1.num_batches_tracked', 'visual_extractor.model.5.0.conv2.weight', 'visual_extractor.model.5.0.bn2.weight', 'visual_extractor.model.5.0.bn2.bias', 'visual_extractor.model.5.0.bn2.running_mean', 'visual_extractor.model.5.0.bn2.running_var', 'visual_extractor.model.5.0.bn2.num_batches_tracked', 'visual_extractor.model.5.0.conv3.weight', 'visual_extractor.model.5.0.bn3.weight', 'visual_extractor.model.5.0.bn3.bias', 'visual_extractor.model.5.0.bn3.running_mean', 'visual_extractor.model.5.0.bn3.running_var', 'visual_extractor.model.5.0.bn3.num_batches_tracked', 'visual_extractor.model.5.0.downsample.0.weight', 'visual_extractor.model.5.0.downsample.1.weight', 'visual_extractor.model.5.0.downsample.1.bias', 'visual_extractor.model.5.0.downsample.1.running_mean', 'visual_extractor.model.5.0.downsample.1.running_var', 'visual_extractor.model.5.0.downsample.1.num_batches_tracked', 'visual_extractor.model.5.1.conv1.weight', 'visual_extractor.model.5.1.bn1.weight', 'visual_extractor.model.5.1.bn1.bias', 'visual_extractor.model.5.1.bn1.running_mean', 'visual_extractor.model.5.1.bn1.running_var', 'visual_extractor.model.5.1.bn1.num_batches_tracked', 'visual_extractor.model.5.1.conv2.weight', 'visual_extractor.model.5.1.bn2.weight', 'visual_extractor.model.5.1.bn2.bias', 'visual_extractor.model.5.1.bn2.running_mean', 'visual_extractor.model.5.1.bn2.running_var', 'visual_extractor.model.5.1.bn2.num_batches_tracked', 'visual_extractor.model.5.1.conv3.weight', 'visual_extractor.model.5.1.bn3.weight', 'visual_extractor.model.5.1.bn3.bias', 'visual_extractor.model.5.1.bn3.running_mean', 'visual_extractor.model.5.1.bn3.running_var', 'visual_extractor.model.5.1.bn3.num_batches_tracked', 'visual_extractor.model.5.2.conv1.weight', 'visual_extractor.model.5.2.bn1.weight', 'visual_extractor.model.5.2.bn1.bias', 'visual_extractor.model.5.2.bn1.running_mean', 'visual_extractor.model.5.2.bn1.running_var', 'visual_extractor.model.5.2.bn1.num_batches_tracked', 'visual_extractor.model.5.2.conv2.weight', 'visual_extractor.model.5.2.bn2.weight', 'visual_extractor.model.5.2.bn2.bias', 'visual_extractor.model.5.2.bn2.running_mean', 'visual_extractor.model.5.2.bn2.running_var', 'visual_extractor.model.5.2.bn2.num_batches_tracked', 'visual_extractor.model.5.2.conv3.weight', 'visual_extractor.model.5.2.bn3.weight', 'visual_extractor.model.5.2.bn3.bias', 'visual_extractor.model.5.2.bn3.running_mean', 'visual_extractor.model.5.2.bn3.running_var', 'visual_extractor.model.5.2.bn3.num_batches_tracked', 'visual_extractor.model.5.3.conv1.weight', 'visual_extractor.model.5.3.bn1.weight', 'visual_extractor.model.5.3.bn1.bias', 'visual_extractor.model.5.3.bn1.running_mean', 'visual_extractor.model.5.3.bn1.running_var', 'visual_extractor.model.5.3.bn1.num_batches_tracked', 'visual_extractor.model.5.3.conv2.weight', 'visual_extractor.model.5.3.bn2.weight', 'visual_extractor.model.5.3.bn2.bias', 'visual_extractor.model.5.3.bn2.running_mean', 'visual_extractor.model.5.3.bn2.running_var', 'visual_extractor.model.5.3.bn2.num_batches_tracked', 'visual_extractor.model.5.3.conv3.weight', 'visual_extractor.model.5.3.bn3.weight', 'visual_extractor.model.5.3.bn3.bias', 'visual_extractor.model.5.3.bn3.running_mean', 'visual_extractor.model.5.3.bn3.running_var', 'visual_extractor.model.5.3.bn3.num_batches_tracked', 'visual_extractor.model.6.0.conv1.weight', 'visual_extractor.model.6.0.bn1.weight', 'visual_extractor.model.6.0.bn1.bias', 'visual_extractor.model.6.0.bn1.running_mean', 'visual_extractor.model.6.0.bn1.running_var', 'visual_extractor.model.6.0.bn1.num_batches_tracked', 'visual_extractor.model.6.0.conv2.weight', 'visual_extractor.model.6.0.bn2.weight', 'visual_extractor.model.6.0.bn2.bias', 'visual_extractor.model.6.0.bn2.running_mean', 'visual_extractor.model.6.0.bn2.running_var', 'visual_extractor.model.6.0.bn2.num_batches_tracked', 'visual_extractor.model.6.0.conv3.weight', 'visual_extractor.model.6.0.bn3.weight', 'visual_extractor.model.6.0.bn3.bias', 'visual_extractor.model.6.0.bn3.running_mean', 'visual_extractor.model.6.0.bn3.running_var', 'visual_extractor.model.6.0.bn3.num_batches_tracked', 'visual_extractor.model.6.0.downsample.0.weight', 'visual_extractor.model.6.0.downsample.1.weight', 'visual_extractor.model.6.0.downsample.1.bias', 'visual_extractor.model.6.0.downsample.1.running_mean', 'visual_extractor.model.6.0.downsample.1.running_var', 'visual_extractor.model.6.0.downsample.1.num_batches_tracked', 'visual_extractor.model.6.1.conv1.weight', 'visual_extractor.model.6.1.bn1.weight', 'visual_extractor.model.6.1.bn1.bias', 'visual_extractor.model.6.1.bn1.running_mean', 'visual_extractor.model.6.1.bn1.running_var', 'visual_extractor.model.6.1.bn1.num_batches_tracked', 'visual_extractor.model.6.1.conv2.weight', 'visual_extractor.model.6.1.bn2.weight', 'visual_extractor.model.6.1.bn2.bias', 'visual_extractor.model.6.1.bn2.running_mean', 'visual_extractor.model.6.1.bn2.running_var', 'visual_extractor.model.6.1.bn2.num_batches_tracked', 'visual_extractor.model.6.1.conv3.weight', 'visual_extractor.model.6.1.bn3.weight', 'visual_extractor.model.6.1.bn3.bias', 'visual_extractor.model.6.1.bn3.running_mean', 'visual_extractor.model.6.1.bn3.running_var', 'visual_extractor.model.6.1.bn3.num_batches_tracked', 'visual_extractor.model.6.2.conv1.weight', 'visual_extractor.model.6.2.bn1.weight', 'visual_extractor.model.6.2.bn1.bias', 'visual_extractor.model.6.2.bn1.running_mean', 'visual_extractor.model.6.2.bn1.running_var', 'visual_extractor.model.6.2.bn1.num_batches_tracked', 'visual_extractor.model.6.2.conv2.weight', 'visual_extractor.model.6.2.bn2.weight', 'visual_extractor.model.6.2.bn2.bias', 'visual_extractor.model.6.2.bn2.running_mean', 'visual_extractor.model.6.2.bn2.running_var', 'visual_extractor.model.6.2.bn2.num_batches_tracked', 'visual_extractor.model.6.2.conv3.weight', 'visual_extractor.model.6.2.bn3.weight', 'visual_extractor.model.6.2.bn3.bias', 'visual_extractor.model.6.2.bn3.running_mean', 'visual_extractor.model.6.2.bn3.running_var', 'visual_extractor.model.6.2.bn3.num_batches_tracked', 'visual_extractor.model.6.3.conv1.weight', 'visual_extractor.model.6.3.bn1.weight', 'visual_extractor.model.6.3.bn1.bias', 'visual_extractor.model.6.3.bn1.running_mean', 'visual_extractor.model.6.3.bn1.running_var', 'visual_extractor.model.6.3.bn1.num_batches_tracked', 'visual_extractor.model.6.3.conv2.weight', 'visual_extractor.model.6.3.bn2.weight', 'visual_extractor.model.6.3.bn2.bias', 'visual_extractor.model.6.3.bn2.running_mean', 'visual_extractor.model.6.3.bn2.running_var', 'visual_extractor.model.6.3.bn2.num_batches_tracked', 'visual_extractor.model.6.3.conv3.weight', 'visual_extractor.model.6.3.bn3.weight', 'visual_extractor.model.6.3.bn3.bias', 'visual_extractor.model.6.3.bn3.running_mean', 'visual_extractor.model.6.3.bn3.running_var', 'visual_extractor.model.6.3.bn3.num_batches_tracked', 'visual_extractor.model.6.4.conv1.weight', 'visual_extractor.model.6.4.bn1.weight', 'visual_extractor.model.6.4.bn1.bias', 'visual_extractor.model.6.4.bn1.running_mean', 'visual_extractor.model.6.4.bn1.running_var', 'visual_extractor.model.6.4.bn1.num_batches_tracked', 'visual_extractor.model.6.4.conv2.weight', 'visual_extractor.model.6.4.bn2.weight', 'visual_extractor.model.6.4.bn2.bias', 'visual_extractor.model.6.4.bn2.running_mean', 'visual_extractor.model.6.4.bn2.running_var', 'visual_extractor.model.6.4.bn2.num_batches_tracked', 'visual_extractor.model.6.4.conv3.weight', 'visual_extractor.model.6.4.bn3.weight', 'visual_extractor.model.6.4.bn3.bias', 'visual_extractor.model.6.4.bn3.running_mean', 'visual_extractor.model.6.4.bn3.running_var', 'visual_extractor.model.6.4.bn3.num_batches_tracked', 'visual_extractor.model.6.5.conv1.weight', 'visual_extractor.model.6.5.bn1.weight', 'visual_extractor.model.6.5.bn1.bias', 'visual_extractor.model.6.5.bn1.running_mean', 'visual_extractor.model.6.5.bn1.running_var', 'visual_extractor.model.6.5.bn1.num_batches_tracked', 'visual_extractor.model.6.5.conv2.weight', 'visual_extractor.model.6.5.bn2.weight', 'visual_extractor.model.6.5.bn2.bias', 'visual_extractor.model.6.5.bn2.running_mean', 'visual_extractor.model.6.5.bn2.running_var', 'visual_extractor.model.6.5.bn2.num_batches_tracked', 'visual_extractor.model.6.5.conv3.weight', 'visual_extractor.model.6.5.bn3.weight', 'visual_extractor.model.6.5.bn3.bias', 'visual_extractor.model.6.5.bn3.running_mean', 'visual_extractor.model.6.5.bn3.running_var', 'visual_extractor.model.6.5.bn3.num_batches_tracked', 'visual_extractor.model.6.6.conv1.weight', 'visual_extractor.model.6.6.bn1.weight', 'visual_extractor.model.6.6.bn1.bias', 'visual_extractor.model.6.6.bn1.running_mean', 'visual_extractor.model.6.6.bn1.running_var', 'visual_extractor.model.6.6.bn1.num_batches_tracked', 'visual_extractor.model.6.6.conv2.weight', 'visual_extractor.model.6.6.bn2.weight', 'visual_extractor.model.6.6.bn2.bias', 'visual_extractor.model.6.6.bn2.running_mean', 'visual_extractor.model.6.6.bn2.running_var', 'visual_extractor.model.6.6.bn2.num_batches_tracked', 'visual_extractor.model.6.6.conv3.weight', 'visual_extractor.model.6.6.bn3.weight', 'visual_extractor.model.6.6.bn3.bias', 'visual_extractor.model.6.6.bn3.running_mean', 'visual_extractor.model.6.6.bn3.running_var', 'visual_extractor.model.6.6.bn3.num_batches_tracked', 'visual_extractor.model.6.7.conv1.weight', 'visual_extractor.model.6.7.bn1.weight', 'visual_extractor.model.6.7.bn1.bias', 'visual_extractor.model.6.7.bn1.running_mean', 'visual_extractor.model.6.7.bn1.running_var', 'visual_extractor.model.6.7.bn1.num_batches_tracked', 'visual_extractor.model.6.7.conv2.weight', 'visual_extractor.model.6.7.bn2.weight', 'visual_extractor.model.6.7.bn2.bias', 'visual_extractor.model.6.7.bn2.running_mean', 'visual_extractor.model.6.7.bn2.running_var', 'visual_extractor.model.6.7.bn2.num_batches_tracked', 'visual_extractor.model.6.7.conv3.weight', 'visual_extractor.model.6.7.bn3.weight', 'visual_extractor.model.6.7.bn3.bias', 'visual_extractor.model.6.7.bn3.running_mean', 'visual_extractor.model.6.7.bn3.running_var', 'visual_extractor.model.6.7.bn3.num_batches_tracked', 'visual_extractor.model.6.8.conv1.weight', 'visual_extractor.model.6.8.bn1.weight', 'visual_extractor.model.6.8.bn1.bias', 'visual_extractor.model.6.8.bn1.running_mean', 'visual_extractor.model.6.8.bn1.running_var', 'visual_extractor.model.6.8.bn1.num_batches_tracked', 'visual_extractor.model.6.8.conv2.weight', 'visual_extractor.model.6.8.bn2.weight', 'visual_extractor.model.6.8.bn2.bias', 'visual_extractor.model.6.8.bn2.running_mean', 'visual_extractor.model.6.8.bn2.running_var', 'visual_extractor.model.6.8.bn2.num_batches_tracked', 'visual_extractor.model.6.8.conv3.weight', 'visual_extractor.model.6.8.bn3.weight', 'visual_extractor.model.6.8.bn3.bias', 'visual_extractor.model.6.8.bn3.running_mean', 'visual_extractor.model.6.8.bn3.running_var', 'visual_extractor.model.6.8.bn3.num_batches_tracked', 'visual_extractor.model.6.9.conv1.weight', 'visual_extractor.model.6.9.bn1.weight', 'visual_extractor.model.6.9.bn1.bias', 'visual_extractor.model.6.9.bn1.running_mean', 'visual_extractor.model.6.9.bn1.running_var', 'visual_extractor.model.6.9.bn1.num_batches_tracked', 'visual_extractor.model.6.9.conv2.weight', 'visual_extractor.model.6.9.bn2.weight', 'visual_extractor.model.6.9.bn2.bias', 'visual_extractor.model.6.9.bn2.running_mean', 'visual_extractor.model.6.9.bn2.running_var', 'visual_extractor.model.6.9.bn2.num_batches_tracked', 'visual_extractor.model.6.9.conv3.weight', 'visual_extractor.model.6.9.bn3.weight', 'visual_extractor.model.6.9.bn3.bias', 'visual_extractor.model.6.9.bn3.running_mean', 'visual_extractor.model.6.9.bn3.running_var', 'visual_extractor.model.6.9.bn3.num_batches_tracked', 'visual_extractor.model.6.10.conv1.weight', 'visual_extractor.model.6.10.bn1.weight', 'visual_extractor.model.6.10.bn1.bias', 'visual_extractor.model.6.10.bn1.running_mean', 'visual_extractor.model.6.10.bn1.running_var', 'visual_extractor.model.6.10.bn1.num_batches_tracked', 'visual_extractor.model.6.10.conv2.weight', 'visual_extractor.model.6.10.bn2.weight', 'visual_extractor.model.6.10.bn2.bias', 'visual_extractor.model.6.10.bn2.running_mean', 'visual_extractor.model.6.10.bn2.running_var', 'visual_extractor.model.6.10.bn2.num_batches_tracked', 'visual_extractor.model.6.10.conv3.weight', 'visual_extractor.model.6.10.bn3.weight', 'visual_extractor.model.6.10.bn3.bias', 'visual_extractor.model.6.10.bn3.running_mean', 'visual_extractor.model.6.10.bn3.running_var', 'visual_extractor.model.6.10.bn3.num_batches_tracked', 'visual_extractor.model.6.11.conv1.weight', 'visual_extractor.model.6.11.bn1.weight', 'visual_extractor.model.6.11.bn1.bias', 'visual_extractor.model.6.11.bn1.running_mean', 'visual_extractor.model.6.11.bn1.running_var', 'visual_extractor.model.6.11.bn1.num_batches_tracked', 'visual_extractor.model.6.11.conv2.weight', 'visual_extractor.model.6.11.bn2.weight', 'visual_extractor.model.6.11.bn2.bias', 'visual_extractor.model.6.11.bn2.running_mean', 'visual_extractor.model.6.11.bn2.running_var', 'visual_extractor.model.6.11.bn2.num_batches_tracked', 'visual_extractor.model.6.11.conv3.weight', 'visual_extractor.model.6.11.bn3.weight', 'visual_extractor.model.6.11.bn3.bias', 'visual_extractor.model.6.11.bn3.running_mean', 'visual_extractor.model.6.11.bn3.running_var', 'visual_extractor.model.6.11.bn3.num_batches_tracked', 'visual_extractor.model.6.12.conv1.weight', 'visual_extractor.model.6.12.bn1.weight', 'visual_extractor.model.6.12.bn1.bias', 'visual_extractor.model.6.12.bn1.running_mean', 'visual_extractor.model.6.12.bn1.running_var', 'visual_extractor.model.6.12.bn1.num_batches_tracked', 'visual_extractor.model.6.12.conv2.weight', 'visual_extractor.model.6.12.bn2.weight', 'visual_extractor.model.6.12.bn2.bias', 'visual_extractor.model.6.12.bn2.running_mean', 'visual_extractor.model.6.12.bn2.running_var', 'visual_extractor.model.6.12.bn2.num_batches_tracked', 'visual_extractor.model.6.12.conv3.weight', 'visual_extractor.model.6.12.bn3.weight', 'visual_extractor.model.6.12.bn3.bias', 'visual_extractor.model.6.12.bn3.running_mean', 'visual_extractor.model.6.12.bn3.running_var', 'visual_extractor.model.6.12.bn3.num_batches_tracked', 'visual_extractor.model.6.13.conv1.weight', 'visual_extractor.model.6.13.bn1.weight', 'visual_extractor.model.6.13.bn1.bias', 'visual_extractor.model.6.13.bn1.running_mean', 'visual_extractor.model.6.13.bn1.running_var', 'visual_extractor.model.6.13.bn1.num_batches_tracked', 'visual_extractor.model.6.13.conv2.weight', 'visual_extractor.model.6.13.bn2.weight', 'visual_extractor.model.6.13.bn2.bias', 'visual_extractor.model.6.13.bn2.running_mean', 'visual_extractor.model.6.13.bn2.running_var', 'visual_extractor.model.6.13.bn2.num_batches_tracked', 'visual_extractor.model.6.13.conv3.weight', 'visual_extractor.model.6.13.bn3.weight', 'visual_extractor.model.6.13.bn3.bias', 'visual_extractor.model.6.13.bn3.running_mean', 'visual_extractor.model.6.13.bn3.running_var', 'visual_extractor.model.6.13.bn3.num_batches_tracked', 'visual_extractor.model.6.14.conv1.weight', 'visual_extractor.model.6.14.bn1.weight', 'visual_extractor.model.6.14.bn1.bias', 'visual_extractor.model.6.14.bn1.running_mean', 'visual_extractor.model.6.14.bn1.running_var', 'visual_extractor.model.6.14.bn1.num_batches_tracked', 'visual_extractor.model.6.14.conv2.weight', 'visual_extractor.model.6.14.bn2.weight', 'visual_extractor.model.6.14.bn2.bias', 'visual_extractor.model.6.14.bn2.running_mean', 'visual_extractor.model.6.14.bn2.running_var', 'visual_extractor.model.6.14.bn2.num_batches_tracked', 'visual_extractor.model.6.14.conv3.weight', 'visual_extractor.model.6.14.bn3.weight', 'visual_extractor.model.6.14.bn3.bias', 'visual_extractor.model.6.14.bn3.running_mean', 'visual_extractor.model.6.14.bn3.running_var', 'visual_extractor.model.6.14.bn3.num_batches_tracked', 'visual_extractor.model.6.15.conv1.weight', 'visual_extractor.model.6.15.bn1.weight', 'visual_extractor.model.6.15.bn1.bias', 'visual_extractor.model.6.15.bn1.running_mean', 'visual_extractor.model.6.15.bn1.running_var', 'visual_extractor.model.6.15.bn1.num_batches_tracked', 'visual_extractor.model.6.15.conv2.weight', 'visual_extractor.model.6.15.bn2.weight', 'visual_extractor.model.6.15.bn2.bias', 'visual_extractor.model.6.15.bn2.running_mean', 'visual_extractor.model.6.15.bn2.running_var', 'visual_extractor.model.6.15.bn2.num_batches_tracked', 'visual_extractor.model.6.15.conv3.weight', 'visual_extractor.model.6.15.bn3.weight', 'visual_extractor.model.6.15.bn3.bias', 'visual_extractor.model.6.15.bn3.running_mean', 'visual_extractor.model.6.15.bn3.running_var', 'visual_extractor.model.6.15.bn3.num_batches_tracked', 'visual_extractor.model.6.16.conv1.weight', 'visual_extractor.model.6.16.bn1.weight', 'visual_extractor.model.6.16.bn1.bias', 'visual_extractor.model.6.16.bn1.running_mean', 'visual_extractor.model.6.16.bn1.running_var', 'visual_extractor.model.6.16.bn1.num_batches_tracked', 'visual_extractor.model.6.16.conv2.weight', 'visual_extractor.model.6.16.bn2.weight', 'visual_extractor.model.6.16.bn2.bias', 'visual_extractor.model.6.16.bn2.running_mean', 'visual_extractor.model.6.16.bn2.running_var', 'visual_extractor.model.6.16.bn2.num_batches_tracked', 'visual_extractor.model.6.16.conv3.weight', 'visual_extractor.model.6.16.bn3.weight', 'visual_extractor.model.6.16.bn3.bias', 'visual_extractor.model.6.16.bn3.running_mean', 'visual_extractor.model.6.16.bn3.running_var', 'visual_extractor.model.6.16.bn3.num_batches_tracked', 'visual_extractor.model.6.17.conv1.weight', 'visual_extractor.model.6.17.bn1.weight', 'visual_extractor.model.6.17.bn1.bias', 'visual_extractor.model.6.17.bn1.running_mean', 'visual_extractor.model.6.17.bn1.running_var', 'visual_extractor.model.6.17.bn1.num_batches_tracked', 'visual_extractor.model.6.17.conv2.weight', 'visual_extractor.model.6.17.bn2.weight', 'visual_extractor.model.6.17.bn2.bias', 'visual_extractor.model.6.17.bn2.running_mean', 'visual_extractor.model.6.17.bn2.running_var', 'visual_extractor.model.6.17.bn2.num_batches_tracked', 'visual_extractor.model.6.17.conv3.weight', 'visual_extractor.model.6.17.bn3.weight', 'visual_extractor.model.6.17.bn3.bias', 'visual_extractor.model.6.17.bn3.running_mean', 'visual_extractor.model.6.17.bn3.running_var', 'visual_extractor.model.6.17.bn3.num_batches_tracked', 'visual_extractor.model.6.18.conv1.weight', 'visual_extractor.model.6.18.bn1.weight', 'visual_extractor.model.6.18.bn1.bias', 'visual_extractor.model.6.18.bn1.running_mean', 'visual_extractor.model.6.18.bn1.running_var', 'visual_extractor.model.6.18.bn1.num_batches_tracked', 'visual_extractor.model.6.18.conv2.weight', 'visual_extractor.model.6.18.bn2.weight', 'visual_extractor.model.6.18.bn2.bias', 'visual_extractor.model.6.18.bn2.running_mean', 'visual_extractor.model.6.18.bn2.running_var', 'visual_extractor.model.6.18.bn2.num_batches_tracked', 'visual_extractor.model.6.18.conv3.weight', 'visual_extractor.model.6.18.bn3.weight', 'visual_extractor.model.6.18.bn3.bias', 'visual_extractor.model.6.18.bn3.running_mean', 'visual_extractor.model.6.18.bn3.running_var', 'visual_extractor.model.6.18.bn3.num_batches_tracked', 'visual_extractor.model.6.19.conv1.weight', 'visual_extractor.model.6.19.bn1.weight', 'visual_extractor.model.6.19.bn1.bias', 'visual_extractor.model.6.19.bn1.running_mean', 'visual_extractor.model.6.19.bn1.running_var', 'visual_extractor.model.6.19.bn1.num_batches_tracked', 'visual_extractor.model.6.19.conv2.weight', 'visual_extractor.model.6.19.bn2.weight', 'visual_extractor.model.6.19.bn2.bias', 'visual_extractor.model.6.19.bn2.running_mean', 'visual_extractor.model.6.19.bn2.running_var', 'visual_extractor.model.6.19.bn2.num_batches_tracked', 'visual_extractor.model.6.19.conv3.weight', 'visual_extractor.model.6.19.bn3.weight', 'visual_extractor.model.6.19.bn3.bias', 'visual_extractor.model.6.19.bn3.running_mean', 'visual_extractor.model.6.19.bn3.running_var', 'visual_extractor.model.6.19.bn3.num_batches_tracked', 'visual_extractor.model.6.20.conv1.weight', 'visual_extractor.model.6.20.bn1.weight', 'visual_extractor.model.6.20.bn1.bias', 'visual_extractor.model.6.20.bn1.running_mean', 'visual_extractor.model.6.20.bn1.running_var', 'visual_extractor.model.6.20.bn1.num_batches_tracked', 'visual_extractor.model.6.20.conv2.weight', 'visual_extractor.model.6.20.bn2.weight', 'visual_extractor.model.6.20.bn2.bias', 'visual_extractor.model.6.20.bn2.running_mean', 'visual_extractor.model.6.20.bn2.running_var', 'visual_extractor.model.6.20.bn2.num_batches_tracked', 'visual_extractor.model.6.20.conv3.weight', 'visual_extractor.model.6.20.bn3.weight', 'visual_extractor.model.6.20.bn3.bias', 'visual_extractor.model.6.20.bn3.running_mean', 'visual_extractor.model.6.20.bn3.running_var', 'visual_extractor.model.6.20.bn3.num_batches_tracked', 'visual_extractor.model.6.21.conv1.weight', 'visual_extractor.model.6.21.bn1.weight', 'visual_extractor.model.6.21.bn1.bias', 'visual_extractor.model.6.21.bn1.running_mean', 'visual_extractor.model.6.21.bn1.running_var', 'visual_extractor.model.6.21.bn1.num_batches_tracked', 'visual_extractor.model.6.21.conv2.weight', 'visual_extractor.model.6.21.bn2.weight', 'visual_extractor.model.6.21.bn2.bias', 'visual_extractor.model.6.21.bn2.running_mean', 'visual_extractor.model.6.21.bn2.running_var', 'visual_extractor.model.6.21.bn2.num_batches_tracked', 'visual_extractor.model.6.21.conv3.weight', 'visual_extractor.model.6.21.bn3.weight', 'visual_extractor.model.6.21.bn3.bias', 'visual_extractor.model.6.21.bn3.running_mean', 'visual_extractor.model.6.21.bn3.running_var', 'visual_extractor.model.6.21.bn3.num_batches_tracked', 'visual_extractor.model.6.22.conv1.weight', 'visual_extractor.model.6.22.bn1.weight', 'visual_extractor.model.6.22.bn1.bias', 'visual_extractor.model.6.22.bn1.running_mean', 'visual_extractor.model.6.22.bn1.running_var', 'visual_extractor.model.6.22.bn1.num_batches_tracked', 'visual_extractor.model.6.22.conv2.weight', 'visual_extractor.model.6.22.bn2.weight', 'visual_extractor.model.6.22.bn2.bias', 'visual_extractor.model.6.22.bn2.running_mean', 'visual_extractor.model.6.22.bn2.running_var', 'visual_extractor.model.6.22.bn2.num_batches_tracked', 'visual_extractor.model.6.22.conv3.weight', 'visual_extractor.model.6.22.bn3.weight', 'visual_extractor.model.6.22.bn3.bias', 'visual_extractor.model.6.22.bn3.running_mean', 'visual_extractor.model.6.22.bn3.running_var', 'visual_extractor.model.6.22.bn3.num_batches_tracked', 'visual_extractor.model.7.0.conv1.weight', 'visual_extractor.model.7.0.bn1.weight', 'visual_extractor.model.7.0.bn1.bias', 'visual_extractor.model.7.0.bn1.running_mean', 'visual_extractor.model.7.0.bn1.running_var', 'visual_extractor.model.7.0.bn1.num_batches_tracked', 'visual_extractor.model.7.0.conv2.weight', 'visual_extractor.model.7.0.bn2.weight', 'visual_extractor.model.7.0.bn2.bias', 'visual_extractor.model.7.0.bn2.running_mean', 'visual_extractor.model.7.0.bn2.running_var', 'visual_extractor.model.7.0.bn2.num_batches_tracked', 'visual_extractor.model.7.0.conv3.weight', 'visual_extractor.model.7.0.bn3.weight', 'visual_extractor.model.7.0.bn3.bias', 'visual_extractor.model.7.0.bn3.running_mean', 'visual_extractor.model.7.0.bn3.running_var', 'visual_extractor.model.7.0.bn3.num_batches_tracked', 'visual_extractor.model.7.0.downsample.0.weight', 'visual_extractor.model.7.0.downsample.1.weight', 'visual_extractor.model.7.0.downsample.1.bias', 'visual_extractor.model.7.0.downsample.1.running_mean', 'visual_extractor.model.7.0.downsample.1.running_var', 'visual_extractor.model.7.0.downsample.1.num_batches_tracked', 'visual_extractor.model.7.1.conv1.weight', 'visual_extractor.model.7.1.bn1.weight', 'visual_extractor.model.7.1.bn1.bias', 'visual_extractor.model.7.1.bn1.running_mean', 'visual_extractor.model.7.1.bn1.running_var', 'visual_extractor.model.7.1.bn1.num_batches_tracked', 'visual_extractor.model.7.1.conv2.weight', 'visual_extractor.model.7.1.bn2.weight', 'visual_extractor.model.7.1.bn2.bias', 'visual_extractor.model.7.1.bn2.running_mean', 'visual_extractor.model.7.1.bn2.running_var', 'visual_extractor.model.7.1.bn2.num_batches_tracked', 'visual_extractor.model.7.1.conv3.weight', 'visual_extractor.model.7.1.bn3.weight', 'visual_extractor.model.7.1.bn3.bias', 'visual_extractor.model.7.1.bn3.running_mean', 'visual_extractor.model.7.1.bn3.running_var', 'visual_extractor.model.7.1.bn3.num_batches_tracked', 'visual_extractor.model.7.2.conv1.weight', 'visual_extractor.model.7.2.bn1.weight', 'visual_extractor.model.7.2.bn1.bias', 'visual_extractor.model.7.2.bn1.running_mean', 'visual_extractor.model.7.2.bn1.running_var', 'visual_extractor.model.7.2.bn1.num_batches_tracked', 'visual_extractor.model.7.2.conv2.weight', 'visual_extractor.model.7.2.bn2.weight', 'visual_extractor.model.7.2.bn2.bias', 'visual_extractor.model.7.2.bn2.running_mean', 'visual_extractor.model.7.2.bn2.running_var', 'visual_extractor.model.7.2.bn2.num_batches_tracked', 'visual_extractor.model.7.2.conv3.weight', 'visual_extractor.model.7.2.bn3.weight', 'visual_extractor.model.7.2.bn3.bias', 'visual_extractor.model.7.2.bn3.running_mean', 'visual_extractor.model.7.2.bn3.running_var', 'visual_extractor.model.7.2.bn3.num_batches_tracked', 'encoder_decoder.memory_matrix', 'encoder_decoder.att_embed.0.weight', 'encoder_decoder.att_embed.0.bias', 'encoder_decoder.cmn.linears.0.weight', 'encoder_decoder.cmn.linears.0.bias', 'encoder_decoder.cmn.linears.1.weight', 'encoder_decoder.cmn.linears.1.bias', 'encoder_decoder.cmn.linears.2.weight', 'encoder_decoder.cmn.linears.2.bias', 'encoder_decoder.cmn.linears.3.weight', 'encoder_decoder.cmn.linears.3.bias', 'encoder_decoder.model.encoder.layers.0.self_attn.linears.0.weight', 'encoder_decoder.model.encoder.layers.0.self_attn.linears.0.bias', 'encoder_decoder.model.encoder.layers.0.self_attn.linears.1.weight', 'encoder_decoder.model.encoder.layers.0.self_attn.linears.1.bias', 'encoder_decoder.model.encoder.layers.0.self_attn.linears.2.weight', 'encoder_decoder.model.encoder.layers.0.self_attn.linears.2.bias', 'encoder_decoder.model.encoder.layers.0.self_attn.linears.3.weight', 'encoder_decoder.model.encoder.layers.0.self_attn.linears.3.bias', 'encoder_decoder.model.encoder.layers.0.feed_forward.w_1.weight', 'encoder_decoder.model.encoder.layers.0.feed_forward.w_1.bias', 'encoder_decoder.model.encoder.layers.0.feed_forward.w_2.weight', 'encoder_decoder.model.encoder.layers.0.feed_forward.w_2.bias', 'encoder_decoder.model.encoder.layers.0.sublayer.0.norm.a_2', 'encoder_decoder.model.encoder.layers.0.sublayer.0.norm.b_2', 'encoder_decoder.model.encoder.layers.0.sublayer.1.norm.a_2', 'encoder_decoder.model.encoder.layers.0.sublayer.1.norm.b_2', 'encoder_decoder.model.encoder.layers.1.self_attn.linears.0.weight', 'encoder_decoder.model.encoder.layers.1.self_attn.linears.0.bias', 'encoder_decoder.model.encoder.layers.1.self_attn.linears.1.weight', 'encoder_decoder.model.encoder.layers.1.self_attn.linears.1.bias', 'encoder_decoder.model.encoder.layers.1.self_attn.linears.2.weight', 'encoder_decoder.model.encoder.layers.1.self_attn.linears.2.bias', 'encoder_decoder.model.encoder.layers.1.self_attn.linears.3.weight', 'encoder_decoder.model.encoder.layers.1.self_attn.linears.3.bias', 'encoder_decoder.model.encoder.layers.1.feed_forward.w_1.weight', 'encoder_decoder.model.encoder.layers.1.feed_forward.w_1.bias', 'encoder_decoder.model.encoder.layers.1.feed_forward.w_2.weight', 'encoder_decoder.model.encoder.layers.1.feed_forward.w_2.bias', 'encoder_decoder.model.encoder.layers.1.sublayer.0.norm.a_2', 'encoder_decoder.model.encoder.layers.1.sublayer.0.norm.b_2', 'encoder_decoder.model.encoder.layers.1.sublayer.1.norm.a_2', 'encoder_decoder.model.encoder.layers.1.sublayer.1.norm.b_2', 'encoder_decoder.model.encoder.layers.2.self_attn.linears.0.weight', 'encoder_decoder.model.encoder.layers.2.self_attn.linears.0.bias', 'encoder_decoder.model.encoder.layers.2.self_attn.linears.1.weight', 'encoder_decoder.model.encoder.layers.2.self_attn.linears.1.bias', 'encoder_decoder.model.encoder.layers.2.self_attn.linears.2.weight', 'encoder_decoder.model.encoder.layers.2.self_attn.linears.2.bias', 'encoder_decoder.model.encoder.layers.2.self_attn.linears.3.weight', 'encoder_decoder.model.encoder.layers.2.self_attn.linears.3.bias', 'encoder_decoder.model.encoder.layers.2.feed_forward.w_1.weight', 'encoder_decoder.model.encoder.layers.2.feed_forward.w_1.bias', 'encoder_decoder.model.encoder.layers.2.feed_forward.w_2.weight', 'encoder_decoder.model.encoder.layers.2.feed_forward.w_2.bias', 'encoder_decoder.model.encoder.layers.2.sublayer.0.norm.a_2', 'encoder_decoder.model.encoder.layers.2.sublayer.0.norm.b_2', 'encoder_decoder.model.encoder.layers.2.sublayer.1.norm.a_2', 'encoder_decoder.model.encoder.layers.2.sublayer.1.norm.b_2', 'encoder_decoder.model.encoder.norm.a_2', 'encoder_decoder.model.encoder.norm.b_2', 'encoder_decoder.model.decoder.layers.0.self_attn.linears.0.weight', 'encoder_decoder.model.decoder.layers.0.self_attn.linears.0.bias', 'encoder_decoder.model.decoder.layers.0.self_attn.linears.1.weight', 'encoder_decoder.model.decoder.layers.0.self_attn.linears.1.bias', 'encoder_decoder.model.decoder.layers.0.self_attn.linears.2.weight', 'encoder_decoder.model.decoder.layers.0.self_attn.linears.2.bias', 'encoder_decoder.model.decoder.layers.0.self_attn.linears.3.weight', 'encoder_decoder.model.decoder.layers.0.self_attn.linears.3.bias', 'encoder_decoder.model.decoder.layers.0.src_attn.linears.0.weight', 'encoder_decoder.model.decoder.layers.0.src_attn.linears.0.bias', 'encoder_decoder.model.decoder.layers.0.src_attn.linears.1.weight', 'encoder_decoder.model.decoder.layers.0.src_attn.linears.1.bias', 'encoder_decoder.model.decoder.layers.0.src_attn.linears.2.weight', 'encoder_decoder.model.decoder.layers.0.src_attn.linears.2.bias', 'encoder_decoder.model.decoder.layers.0.src_attn.linears.3.weight', 'encoder_decoder.model.decoder.layers.0.src_attn.linears.3.bias', 'encoder_decoder.model.decoder.layers.0.feed_forward.w_1.weight', 'encoder_decoder.model.decoder.layers.0.feed_forward.w_1.bias', 'encoder_decoder.model.decoder.layers.0.feed_forward.w_2.weight', 'encoder_decoder.model.decoder.layers.0.feed_forward.w_2.bias', 'encoder_decoder.model.decoder.layers.0.sublayer.0.norm.a_2', 'encoder_decoder.model.decoder.layers.0.sublayer.0.norm.b_2', 'encoder_decoder.model.decoder.layers.0.sublayer.1.norm.a_2', 'encoder_decoder.model.decoder.layers.0.sublayer.1.norm.b_2', 'encoder_decoder.model.decoder.layers.0.sublayer.2.norm.a_2', 'encoder_decoder.model.decoder.layers.0.sublayer.2.norm.b_2', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.0.0.linear.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.0.0.linear.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.0.0.norm.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.0.0.norm.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.0.1.linear.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.0.1.linear.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.0.1.norm.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.0.1.norm.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.1.0.linear.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.1.0.linear.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.1.0.norm.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.1.0.norm.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.1.1.linear.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.1.1.linear.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.1.1.norm.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.mlps.1.1.norm.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.linear.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.linear.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.norm.a_2', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.norm.b_2', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.src_atten.linears.0.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.src_atten.linears.0.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.src_atten.linears.1.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.src_atten.linears.1.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.src_atten.linears.2.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.src_atten.linears.2.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.src_atten.linears.3.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.src_atten.linears.3.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.rep_atten.linears.0.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.rep_atten.linears.0.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.rep_atten.linears.1.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.rep_atten.linears.1.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.rep_atten.linears.2.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.rep_atten.linears.2.bias', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.rep_atten.linears.3.weight', 'encoder_decoder.model.decoder.layers.0.textCrossAlignModule.rep_atten.linears.3.bias', 'encoder_decoder.model.decoder.layers.1.self_attn.linears.0.weight', 'encoder_decoder.model.decoder.layers.1.self_attn.linears.0.bias', 'encoder_decoder.model.decoder.layers.1.self_attn.linears.1.weight', 'encoder_decoder.model.decoder.layers.1.self_attn.linears.1.bias', 'encoder_decoder.model.decoder.layers.1.self_attn.linears.2.weight', 'encoder_decoder.model.decoder.layers.1.self_attn.linears.2.bias', 'encoder_decoder.model.decoder.layers.1.self_attn.linears.3.weight', 'encoder_decoder.model.decoder.layers.1.self_attn.linears.3.bias', 'encoder_decoder.model.decoder.layers.1.src_attn.linears.0.weight', 'encoder_decoder.model.decoder.layers.1.src_attn.linears.0.bias', 'encoder_decoder.model.decoder.layers.1.src_attn.linears.1.weight', 'encoder_decoder.model.decoder.layers.1.src_attn.linears.1.bias', 'encoder_decoder.model.decoder.layers.1.src_attn.linears.2.weight', 'encoder_decoder.model.decoder.layers.1.src_attn.linears.2.bias', 'encoder_decoder.model.decoder.layers.1.src_attn.linears.3.weight', 'encoder_decoder.model.decoder.layers.1.src_attn.linears.3.bias', 'encoder_decoder.model.decoder.layers.1.feed_forward.w_1.weight', 'encoder_decoder.model.decoder.layers.1.feed_forward.w_1.bias', 'encoder_decoder.model.decoder.layers.1.feed_forward.w_2.weight', 'encoder_decoder.model.decoder.layers.1.feed_forward.w_2.bias', 'encoder_decoder.model.decoder.layers.1.sublayer.0.norm.a_2', 'encoder_decoder.model.decoder.layers.1.sublayer.0.norm.b_2', 'encoder_decoder.model.decoder.layers.1.sublayer.1.norm.a_2', 'encoder_decoder.model.decoder.layers.1.sublayer.1.norm.b_2', 'encoder_decoder.model.decoder.layers.1.sublayer.2.norm.a_2', 'encoder_decoder.model.decoder.layers.1.sublayer.2.norm.b_2', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.0.0.linear.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.0.0.linear.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.0.0.norm.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.0.0.norm.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.0.1.linear.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.0.1.linear.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.0.1.norm.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.0.1.norm.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.1.0.linear.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.1.0.linear.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.1.0.norm.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.1.0.norm.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.1.1.linear.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.1.1.linear.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.1.1.norm.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.mlps.1.1.norm.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.linear.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.linear.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.norm.a_2', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.norm.b_2', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.src_atten.linears.0.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.src_atten.linears.0.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.src_atten.linears.1.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.src_atten.linears.1.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.src_atten.linears.2.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.src_atten.linears.2.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.src_atten.linears.3.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.src_atten.linears.3.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.rep_atten.linears.0.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.rep_atten.linears.0.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.rep_atten.linears.1.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.rep_atten.linears.1.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.rep_atten.linears.2.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.rep_atten.linears.2.bias', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.rep_atten.linears.3.weight', 'encoder_decoder.model.decoder.layers.1.textCrossAlignModule.rep_atten.linears.3.bias', 'encoder_decoder.model.decoder.layers.2.self_attn.linears.0.weight', 'encoder_decoder.model.decoder.layers.2.self_attn.linears.0.bias', 'encoder_decoder.model.decoder.layers.2.self_attn.linears.1.weight', 'encoder_decoder.model.decoder.layers.2.self_attn.linears.1.bias', 'encoder_decoder.model.decoder.layers.2.self_attn.linears.2.weight', 'encoder_decoder.model.decoder.layers.2.self_attn.linears.2.bias', 'encoder_decoder.model.decoder.layers.2.self_attn.linears.3.weight', 'encoder_decoder.model.decoder.layers.2.self_attn.linears.3.bias', 'encoder_decoder.model.decoder.layers.2.src_attn.linears.0.weight', 'encoder_decoder.model.decoder.layers.2.src_attn.linears.0.bias', 'encoder_decoder.model.decoder.layers.2.src_attn.linears.1.weight', 'encoder_decoder.model.decoder.layers.2.src_attn.linears.1.bias', 'encoder_decoder.model.decoder.layers.2.src_attn.linears.2.weight', 'encoder_decoder.model.decoder.layers.2.src_attn.linears.2.bias', 'encoder_decoder.model.decoder.layers.2.src_attn.linears.3.weight', 'encoder_decoder.model.decoder.layers.2.src_attn.linears.3.bias', 'encoder_decoder.model.decoder.layers.2.feed_forward.w_1.weight', 'encoder_decoder.model.decoder.layers.2.feed_forward.w_1.bias', 'encoder_decoder.model.decoder.layers.2.feed_forward.w_2.weight', 'encoder_decoder.model.decoder.layers.2.feed_forward.w_2.bias', 'encoder_decoder.model.decoder.layers.2.sublayer.0.norm.a_2', 'encoder_decoder.model.decoder.layers.2.sublayer.0.norm.b_2', 'encoder_decoder.model.decoder.layers.2.sublayer.1.norm.a_2', 'encoder_decoder.model.decoder.layers.2.sublayer.1.norm.b_2', 'encoder_decoder.model.decoder.layers.2.sublayer.2.norm.a_2', 'encoder_decoder.model.decoder.layers.2.sublayer.2.norm.b_2', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.0.0.linear.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.0.0.linear.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.0.0.norm.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.0.0.norm.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.0.1.linear.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.0.1.linear.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.0.1.norm.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.0.1.norm.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.1.0.linear.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.1.0.linear.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.1.0.norm.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.1.0.norm.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.1.1.linear.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.1.1.linear.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.1.1.norm.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.mlps.1.1.norm.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.linear.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.linear.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.norm.a_2', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.norm.b_2', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.src_atten.linears.0.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.src_atten.linears.0.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.src_atten.linears.1.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.src_atten.linears.1.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.src_atten.linears.2.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.src_atten.linears.2.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.src_atten.linears.3.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.src_atten.linears.3.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.rep_atten.linears.0.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.rep_atten.linears.0.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.rep_atten.linears.1.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.rep_atten.linears.1.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.rep_atten.linears.2.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.rep_atten.linears.2.bias', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.rep_atten.linears.3.weight', 'encoder_decoder.model.decoder.layers.2.textCrossAlignModule.rep_atten.linears.3.bias', 'encoder_decoder.model.decoder.norm.a_2', 'encoder_decoder.model.decoder.norm.b_2', 'encoder_decoder.model.src_embed.0.pe', 'encoder_decoder.model.tgt_embed.0.lut.weight', 'encoder_decoder.model.tgt_embed.1.pe', 'encoder_decoder.model.cmn.linears.0.weight', 'encoder_decoder.model.cmn.linears.0.bias', 'encoder_decoder.model.cmn.linears.1.weight', 'encoder_decoder.model.cmn.linears.1.bias', 'encoder_decoder.model.cmn.linears.2.weight', 'encoder_decoder.model.cmn.linears.2.bias', 'encoder_decoder.model.cmn.linears.3.weight', 'encoder_decoder.model.cmn.linears.3.bias', 'encoder_decoder.logit.weight', 'encoder_decoder.logit.bias', 'ImageFA.mlps.0.0.linear.weight', 'ImageFA.mlps.0.0.linear.bias', 'ImageFA.mlps.0.0.norm.weight', 'ImageFA.mlps.0.0.norm.bias', 'ImageFA.mlps.0.0.norm.running_mean', 'ImageFA.mlps.0.0.norm.running_var', 'ImageFA.mlps.0.0.norm.num_batches_tracked', 'ImageFA.mlps.0.1.linear.weight', 'ImageFA.mlps.0.1.linear.bias', 'ImageFA.mlps.0.1.norm.weight', 'ImageFA.mlps.0.1.norm.bias', 'ImageFA.mlps.0.1.norm.running_mean', 'ImageFA.mlps.0.1.norm.running_var', 'ImageFA.mlps.0.1.norm.num_batches_tracked', 'ImageFA.mlps.1.0.linear.weight', 'ImageFA.mlps.1.0.linear.bias', 'ImageFA.mlps.1.0.norm.weight', 'ImageFA.mlps.1.0.norm.bias', 'ImageFA.mlps.1.0.norm.running_mean', 'ImageFA.mlps.1.0.norm.running_var', 'ImageFA.mlps.1.0.norm.num_batches_tracked', 'ImageFA.mlps.1.1.linear.weight', 'ImageFA.mlps.1.1.linear.bias', 'ImageFA.mlps.1.1.norm.weight', 'ImageFA.mlps.1.1.norm.bias', 'ImageFA.mlps.1.1.norm.running_mean', 'ImageFA.mlps.1.1.norm.running_var', 'ImageFA.mlps.1.1.norm.num_batches_tracked', 'ImageFA.linear1.weight', 'ImageFA.linear1.bias', 'ImageFA.norm.a_2', 'ImageFA.norm.b_2'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight.get(\"ImageFA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "mask = nn.Parameter(torch.normal(mean=1,std=0.05,size=(49, 49)), requires_grad=False)\n",
    "mask_v = torch.randn(49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\anaconda\\envs\\r2gencmn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "mask = nn.Softmax()(mask + mask_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[:32, -32:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0, 4.0, 0.0, 4.0, 2.0, 1.0, 2.0, 1.0, 3.0, 1.0, 3.0, 3.0, 4.0, 2.0, 1.0, 2.0, 7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 1.0, 4.0, 2.0, 6.0, 0.0, 2.0, 5.0, 1.0, 7.0, 0.0, 7.0, 2.0, 1.0, 2.0, 5.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 7.0, 1.0, 4.0, 1.0, 9.0, 3.0, 0.0, 1.0, 1.0, 2.0, 4.0, 2.0, 9.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 7.0, 1.0, 1.0, 3.0, 13.0, 2.0, 2.0, 2.0, 1.0, 1.0, 6.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 4.0, 0.0, 2.0, 1.0, 0.0, 0.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 5.0, 1.0, 1.0, 2.0, 3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 5.0, 1.0, 1.0, 0.0, 3.0, 1.0, 2.0, 5.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 3.0, 0.0, 4.0, 3.0, 2.0, 0.0, 3.0, 0.0, 1.0, 9.0, 10.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 3.0, 0.0, 1.0, 3.0, 4.0, 4.0, 1.0, 2.0, 13.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 3.0, 0.0, 1.0, 2.0, 7.0, 0.0, 6.0, 4.0, 5.0, 7.0, 1.0, 0.0, 1.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 1.0, 2.0, 1.0, 4.0, 1.0, 3.0, 2.0, 12.0, 0.0, 0.0, 3.0, 3.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 8.0, 0.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 3.0, 0.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 2.0, 0.0, 4.0, 0.0, 0.0, 2.0, 3.0, 2.0, 2.0, 4.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 3.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 3.0, 0.0, 0.0, 1.0, 12.0, 1.0, 1.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 0.0, 3.0, 1.0, 3.0, 3.0, 1.0, 0.0, 2.0, 1.0, 1.0, 3.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 0.0, 0.0, 5.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 3.0, 4.0, 1.0, 10.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 3.0, 1.0, 2.0, 0.0, 3.0, 1.0, 1.0, 2.0, 0.0, 2.0, 1.0, 5.0, 0.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 5.0, 1.0, 3.0, 2.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 7.0, 2.0, 3.0, 1.0, 1.0, 0.0, 3.0, 6.0, 9.0, 2.0, 1.0, 11.0, 2.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 4.0, 3.0, 1.0, 1.0, 2.0, 0.0, 1.0, 2.0, 4.0, 1.0, 0.0, 1.0, 4.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 7.0, 2.0, 1.0, 1.0, 2.0, 6.0, 6.0, 1.0, 2.0, 1.0, 3.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 0.0, 18.0, 8.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 9.0, 2.0, 6.0, 5.0, 2.0, 0.0, 0.0, 1.0, 2.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 5.0, 1.0, 1.0, 1.0, 2.0, 3.0, 1.0, 0.0, 4.0, 3.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 5.0, 2.0, 6.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 2.0, 4.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 4.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 3.0, 3.0, 0.0, 4.0, 1.0, 7.0, 3.0, 7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 1.0, 1.0, 0.0, 3.0, 1.0, 2.0, 1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 4.0, 5.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 3.0, 1.0, 17.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 6.0, 1.0, 1.0, 16.0, 2.0, 1.0, 4.0, 3.0, 1.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 1.0, 1.0, 6.0, 0.0, 2.0, 3.0, 1.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 3.0, 4.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 3.0, 1.0, 6.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 4.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 13.0, 3.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 1.0, 2.0, 2.0, 0.0, 3.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 5.0, 11.0, 2.0, 2.0, 2.0, 1.0, 3.0, 2.0, 1.0, 2.0, 4.0, 1.0, 1.0, 2.0, 3.0, 0.0, 0.0, 1.0, 1.0, 5.0, 2.0, 1.0, 0.0, 3.0, 3.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 3.0, 2.0, 1.0, 3.0, 0.0, 15.000001, 3.0, 1.0, 1.0, 4.0, 3.0, 4.0, 0.0, 1.0, 2.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 6.0, 1.0, 3.0, 0.0, 7.0, 1.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 2.0, 5.0, 0.0, 4.0, 5.0, 1.0, 1.0, 6.0, 6.0, 2.0, 3.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.0, 1.0, 4.0, 2.0, 1.0, 1.0, 7.0, 1.0, 0.0, 1.0, 2.0, 2.0, 0.0, 0.0, 4.0, 2.0, 1.0, 3.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 4.0, 9.0, 0.0, 4.0, 0.0, 1.0, 0.0, 3.0, 1.0, 3.0, 0.0, 4.0, 21.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 4.0, 4.0, 7.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 13.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 25.0, 1.0, 1.0, 0.0, 8.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 3.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 8.0, 2.0, 1.0, 3.0, 1.0, 4.0, 1.0, 2.0, 3.0, 1.0, 0.0, 5.0, 0.0, 1.0, 1.0, 10.0, 2.0, 1.0, 0.0, 1.0, 1.0, 9.0, 1.0, 1.0, 5.0, 1.0, 1.0, 0.0, 6.0, 1.0, 1.0, 3.0, 3.0, 3.0, 2.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 0.0, 7.0, 3.0, 1.0, 0.0, 3.0, 2.0, 1.0, 4.0, 2.0, 3.0, 1.0, 4.0, 4.0, 6.0, 0.0, 0.0, 3.0, 1.0, 1.0, 1.0, 12.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 20.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 4.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 5.0, 0.0, 3.0, 3.0, 2.0, 2.0, 9.0, 1.0, 1.0, 3.0, 5.0, 1.0, 1.0, 2.0, 1.0, 0.0, 3.0, 4.0, 6.0, 0.0, 1.0, 0.0, 2.0, 1.0, 4.0, 0.0, 3.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 3.0, 7.0, 1.0, 2.0, 4.0, 2.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 4.0, 3.0, 4.0, 1.0, 0.0, 3.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 8.0, 1.0, 3.0, 1.0, 2.0, 6.0, 3.0, 1.0, 1.0, 1.0, 3.0, 5.0, 10.0, 1.0, 0.0, 5.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 3.0, 2.0, 2.0, 11.0, 2.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 3.0, 3.0, 2.0, 4.0, 0.0, 0.0, 0.0, 3.0, 11.0, 5.0, 5.0, 1.0, 1.0, 0.0, 1.0, 13.0, 0.0, 0.0, 1.0, 0.0, 2.0, 3.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 4.0, 1.0, 2.0, 5.0, 6.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 7.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 6.0, 1.0, 1.0, 0.0, 1.0, 3.0, 1.0, 0.0, 0.0, 3.0, 2.0, 1.0, 3.0, 7.0, 7.0, 0.0, 9.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 2.0, 5.0, 1.0, 7.0, 0.0, 3.0, 0.0, 1.0, 1.0, 4.0, 0.0, 2.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 15.000001, 18.0, 0.0, 0.0, 2.0, 2.0, 0.0, 3.0, 3.0, 1.0, 3.0, 15.000001, 3.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 1.0, 3.0, 0.0, 3.0, 0.0, 0.0, 2.0, 21.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 18.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 3.0, 4.0, 1.0, 1.0, 0.0, 3.0, 2.0, 1.0, 3.0, 1.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 0.0, 6.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 4.0, 3.0, 1.0, 1.0, 0.0, 8.0, 2.0, 4.0, 0.0, 2.0, 2.0, 6.0, 0.0, 2.0, 0.0, 9.0, 3.0, 1.0, 6.0, 3.0, 0.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 1.0, 3.0, 2.0, 0.0, 9.0, 1.0, 1.0, 1.0, 0.0, 2.0, 10.0, 0.0, 1.0, 1.0, 1.0, 0.0, 6.0, 0.0, 3.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 0.0, 1.0, 2.0, 4.0, 7.0, 1.0, 1.0, 1.0, 2.0, 7.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 15.000001, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 4.0, 2.0, 1.0, 4.0, 3.0, 4.0, 0.0, 4.0, 1.0, 2.0, 0.0, 0.0, 3.0, 1.0, 6.0, 1.0, 0.0, 0.0, 0.0, 2.0, 3.0, 2.0, 1.0, 1.0, 2.0, 1.0, 3.0, 2.0, 0.0, 3.0, 0.0, 5.0, 1.0, 1.0, 1.0, 13.0, 1.0, 0.0, 8.0, 1.0, 0.0, 1.0, 4.0, 3.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 4.0, 2.0, 1.0, 3.0, 3.0, 1.0, 4.0, 1.0, 1.0, 1.0, 4.0, 3.0, 1.0, 0.0, 2.0, 3.0, 1.0, 2.0, 5.0, 2.0, 2.0, 4.0, 0.0, 3.0, 1.0, 3.0, 1.0, 0.0, 1.0, 2.0, 1.0, 3.0, 2.0, 1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, 11.0, 1.0, 1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(list(mask.flatten().numpy().round(2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2gencmn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
